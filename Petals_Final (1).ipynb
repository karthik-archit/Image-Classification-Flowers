{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4536,"status":"ok","timestamp":1665138283149,"user":{"displayName":"Karthik Archit","userId":"11149765067007590192"},"user_tz":-330},"id":"nCrGz0bwjdtc","outputId":"682ed31e-a5b3-4376-9c89-2b2fb716d7b5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n","Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n","Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (6.1.2)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2022.9.24)\n","Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.64.1)\n","Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n"]}],"source":["!pip install kaggle\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z5xgzffglvze"},"outputs":[],"source":["import os\n","os.environ['KAGGLE_CONFIG_DIR'] = \"/content/drive/MyDrive/Petals\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":46346,"status":"ok","timestamp":1663521624547,"user":{"displayName":"Karthik Archit","userId":"11149765067007590192"},"user_tz":-330},"id":"DTukQYUWmN_Z","outputId":"c97b0c96-1a12-40af-8a6d-ba7c56badc69"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading tpu-getting-started.zip to /content\n","100% 4.77G/4.79G [00:45<00:00, 134MB/s]\n","100% 4.79G/4.79G [00:45<00:00, 113MB/s]\n"]}],"source":["!kaggle competitions download -c tpu-getting-started"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tt_atXf2qIKM"},"outputs":[],"source":["!unzip \"/content/drive/MyDrive/tpu-getting-started.zip\" -d \"/content/drive/MyDrive/Petals/\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VMGunCO84wq5"},"outputs":[],"source":["from tensorflow.keras.layers import Dense,LSTM,Embedding,Dropout,Bidirectional,GRU,Flatten,BatchNormalization,GlobalAveragePooling2D\n","from tensorflow.keras import layers\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n","from tensorflow.keras import preprocessing\n","from tensorflow.keras import optimizers\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras import regularizers\n","import pandas as pd \n","import numpy as np\n","from sklearn.feature_extraction import FeatureHasher\n","from tensorflow.keras.metrics import MeanSquaredLogarithmicError\n","from tensorflow.keras.applications import InceptionV3,ResNet50V2,EfficientNetB2\n","import os\n","import tensorflow as tf\n","from tensorflow import keras"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vI9SN_ZkRxdD"},"outputs":[],"source":["GCS_DS_PATH = '/content/drive/MyDrive/Colab Notebooks/Petals'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZJR1GbwiRiOQ"},"outputs":[],"source":["IMAGE_SIZE = [224, 224]\n","EPOCHS = 100\n","BATCH_SIZE = 64\n","NUM_TRAINING_IMAGES = 12753\n","NUM_TEST_IMAGES = 7382\n","STEPS_PER_EPOCH = NUM_TRAINING_IMAGES // BATCH_SIZE\n","AUTOTUNE = tf.data.AUTOTUNE"]},{"cell_type":"markdown","metadata":{"id":"Hr1zSYYwQjpy"},"source":["Data augumentation is used , as there a lot of classes for classification and with a small dataset , data agumentation can help produce randomly transformed images of acutal images and are classified to the same label. This helps the model to extract key features more precessively , even when some parts of the images are missing or have different contrast or translated along both the axis or zoomed in etc. It essesntially helps us increase our training dataset. Keras preprocessing layers are used and they are applied to the datasets during the time of training.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AWGqybERhyQZ"},"outputs":[],"source":["data_augmentation = tf.keras.Sequential([\n","  layers.RandomFlip(\"horizontal_and_vertical\"),\n","  layers.RandomRotation(0.2),\n","  layers.RandomTranslation(0.2,0.2,fill_mode=\"reflect\"),\n","  # layers.RandomContrast(0.3),\n","  layers.RandomZoom(0.1)\n","])"]},{"cell_type":"markdown","metadata":{"id":"WwVV_L-ISoxk"},"source":["Random erasing function is used to randomly erase a certain part of the image , creating a black mask. The function first gets the dimensions of the image and using sh and sl which correspond to max and min ratio of total image area and computes the upper and lower bound of possible erasing area. Then the maximum possible height and width of the erasing area is found. Then the position of the erasing is area is randomly found and finally it is applied onto the image. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uqo71D7sP4FA"},"outputs":[],"source":["def random_erasing(img, sl=0.1, sh=0.2, rl=0.4, p=0.3):\n","    h = tf.shape(img)[0]\n","    w = tf.shape(img)[1]\n","    c = tf.shape(img)[2]\n","    origin_area = tf.cast(h*w, tf.float32)\n","\n","    e_size_l = tf.cast(tf.round(tf.sqrt(origin_area * sl * rl)), tf.int32)\n","    e_size_h = tf.cast(tf.round(tf.sqrt(origin_area * sh / rl)), tf.int32)\n","\n","    e_height_h = tf.minimum(e_size_h, h)\n","    e_width_h = tf.minimum(e_size_h, w)\n","\n","    erase_height = tf.random.uniform(shape=[], minval=e_size_l, maxval=e_height_h, dtype=tf.int32)\n","    erase_width = tf.random.uniform(shape=[], minval=e_size_l, maxval=e_width_h, dtype=tf.int32)\n","\n","    erase_area = tf.zeros(shape=[erase_height, erase_width, c])\n","    erase_area = tf.cast(erase_area, tf.uint8)\n","\n","    pad_h = h - erase_height\n","    pad_top = tf.random.uniform(shape=[], minval=0, maxval=pad_h, dtype=tf.int32)\n","    pad_bottom = pad_h - pad_top\n","\n","    pad_w = w - erase_width\n","    pad_left = tf.random.uniform(shape=[], minval=0, maxval=pad_w, dtype=tf.int32)\n","    pad_right = pad_w - pad_left\n","\n","    erase_mask = tf.pad([erase_area], [[0,0],[pad_top, pad_bottom], [pad_left, pad_right], [0,0]], constant_values=1)\n","    erase_mask = tf.squeeze(erase_mask, axis=0)\n","    erased_img = tf.multiply(tf.cast(img,tf.float32), tf.cast(erase_mask, tf.float32))\n","\n","    return tf.cond(tf.random.uniform([], 0, 1) > p, lambda: tf.cast(img, img.dtype), lambda:  tf.cast(erased_img, img.dtype))"]},{"cell_type":"markdown","metadata":{"id":"RX-6KKtRdebE"},"source":["This function is used to decode the image from the tensor and convert the pixel values between 0,1."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"im_yNeG9dct6"},"outputs":[],"source":["def decode_image(image_data):\n","    image = tf.image.decode_jpeg(image_data, channels=3)\n","    image = tf.cast(image, tf.float32) / 255.0  # convert image to floats in [0, 1] range\n","    image = tf.reshape(image, [*IMAGE_SIZE, 3]) # explicit size needed for TPU\n","    return image\n"]},{"cell_type":"markdown","metadata":{"id":"-EoUYAW8dqI2"},"source":["This functions maps each of the image obtained from the tfrecord , which contains the image and label as tensors to their respective types and decode image is called to decode the image."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SXztzE1tdqlf"},"outputs":[],"source":["def read_labeled_tfrecord(example):\n","    LABELED_TFREC_FORMAT = {\n","        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n","        \"class\": tf.io.FixedLenFeature([], tf.int64),  # shape [] means single element\n","    }\n","    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n","    image = decode_image(example['image'])\n","    label = tf.cast(example['class'], tf.int32)\n","    return image, label # returns a dataset of (image, label) pairs"]},{"cell_type":"markdown","metadata":{"id":"1BZjBwa3e4Td"},"source":["This function is used to load the datasets from the tfrecords files , based on labeled value ie , whether it is train or test data the appropriate function for mapping the records is called ."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IfLi_5V6e4pQ"},"outputs":[],"source":["def load_dataset(filenames, labeled=True, ordered=False):\n","\n","    ignore_order = tf.data.Options()\n","    if not ordered:\n","        ignore_order.experimental_deterministic = False\n","\n","    dataset = tf.data.TFRecordDataset(filenames) \n","    dataset = dataset.with_options(ignore_order) \n","    dataset = dataset.map(read_labeled_tfrecord if labeled else read_unlabeled_tfrecord,num_parallel_calls = AUTOTUNE)\n","\n","\n","    return dataset"]},{"cell_type":"markdown","metadata":{"id":"UANPnwSlfWRG"},"source":["The following functions load the files from the local directory and get the tfrecords mapped to the corresponding types using the above functions. Augumentation and erasing functions are applied to the function using the map function of tf datasets. Repeat is used as the dataset has to fetch the values for every epoch , and shuffle enables shuffling the data in the buffer. Prefetch is called so when training of one epoch is going on simultaneously the elements for next epoch or fetched to the buffer , the buffer size is dynamically chosen using Autotune depending upon the availablility of resources."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"--OqvDFTfWxG"},"outputs":[],"source":["def get_training_dataset():\n","    dataset = load_dataset(tf.io.gfile.glob(GCS_DS_PATH + '/tfrecords-jpeg-224x224/train/*.tfrec'), labeled=True)\n","    dataset = dataset.map(lambda x, y: (data_augmentation(x, training=True), y), \n","                num_parallel_calls=AUTOTUNE)\n","    dataset = dataset.map(lambda x, y: (random_erasing(x), y), \n","                num_parallel_calls=AUTOTUNE)\n","    dataset = dataset.repeat()\n","    dataset = dataset.shuffle(2048)\n","    dataset = dataset.batch(BATCH_SIZE)\n","  \n","    return dataset.prefetch(buffer_size=AUTOTUNE)\n","\n","def get_validation_dataset():\n","    dataset = load_dataset(tf.io.gfile.glob(GCS_DS_PATH + '/tfrecords-jpeg-224x224/val/*.tfrec'), labeled=True, ordered=False)\n","    dataset = dataset.batch(BATCH_SIZE)\n","    dataset = dataset.cache()\n","    return dataset\n","    "]},{"cell_type":"markdown","metadata":{"id":"foL3pObziUnh"},"source":["Getting our training and validation Datasets."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wPNCl3SuiVCb"},"outputs":[],"source":["training_dataset = get_training_dataset()\n","validation_dataset = get_validation_dataset()"]},{"cell_type":"markdown","metadata":{"id":"ZhdljhfClRM3"},"source":["Inception V3 is used with the weights of imagenet , include top is set to false because we will provide our own head to our model as the classification is different when comapred to imagenet. The first 249 layers are not trained. An another way would be training our head by freezing the whole inception model and then fine tuning it  by unfreezing a few layers . It can also be trained using a custom training loop if required. But for simplicity im training few of the higher layers of the inception model as a part of the main train run."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3895,"status":"ok","timestamp":1667063545839,"user":{"displayName":"Karthik Archit","userId":"11149765067007590192"},"user_tz":-330},"id":"hXqzMfApSVtX","outputId":"80865ab7-69e6-444e-f0a5-0d7116f3d48b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n","87910968/87910968 [==============================] - 2s 0us/step\n"]}],"source":["inception = InceptionV3(weights='imagenet', include_top=False ,input_shape=[*IMAGE_SIZE, 3])\n","\n","for layer in inception.layers[:249]:\n","   layer.trainable = False\n","for layer in inception.layers[249:]:\n","   layer.trainable = True"]},{"cell_type":"markdown","metadata":{"id":"foXVNZ5imtLC"},"source":["Our Model is created , again for simplicity purpose im using the sequntial model and buling feed forward network as our head. Loss is sparse_categorical_crossentropy as we are doing multi-label classification. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ae5UpU6rS5rf"},"outputs":[],"source":["model = Sequential()\n","model.add(inception)\n","model.add(layers.Flatten())\n","model.add(Dense(512,activation='relu'))\n","model.add(layers.Dropout(0.5))\n","model.add(Dense(256,activation='relu'))\n","model.add(layers.Dropout(0.5))\n","model.add(Dense(104,activation='softmax'))\n","model.compile(optimizer=optimizers.Adam(learning_rate=0.001),\n","    loss = 'sparse_categorical_crossentropy',\n","    metrics=['sparse_categorical_accuracy'])"]},{"cell_type":"markdown","metadata":{"id":"cHZpspuanKwE"},"source":["The appropriate callbacks are set "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wwb8aKhp-Fyn"},"outputs":[],"source":["callbacks_list = [\n","tf.keras.callbacks.ModelCheckpoint(\n","    filepath='/tmp/checkpoint',\n","    save_weights_only=True,\n","    monitor='val_sparse_categorical_accuracy',\n","    mode='max',\n","    save_best_only=True),\n","keras.callbacks.ReduceLROnPlateau(\n","monitor='sparse_categorical_accuracy',\n","factor=0.1,\n","patience=3,\n",")\n","]"]},{"cell_type":"markdown","metadata":{"id":"VAPVqrfdnPR9"},"source":["Finally fitting our model "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"la3iD3H6UWjX"},"outputs":[],"source":["historical = model.fit(training_dataset, \n","          steps_per_epoch=STEPS_PER_EPOCH, \n","          epochs=EPOCHS, \n","          validation_data=validation_dataset,callbacks=callbacks_list)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3822,"status":"ok","timestamp":1667068367953,"user":{"displayName":"Karthik Archit","userId":"11149765067007590192"},"user_tz":-330},"id":"xxy36uEk62Jk","colab":{"base_uri":"https://localhost:8080/"},"outputId":"631e91c7-0335-44de-dc4a-1f7e1ff79b15"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n","94668760/94668760 [==============================] - 1s 0us/step\n"]}],"source":["resnet = ResNet50V2(weights='imagenet', include_top=False ,input_shape=[*IMAGE_SIZE, 3]) \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0dX1P9k47OhO"},"outputs":[],"source":["for layer in resnet.layers[:80]:\n","   layer.trainable = False\n","for layer in resnet.layers[81:]:\n","   layer.trainable = True"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PNtBqHr59uXd"},"outputs":[],"source":["model2 = Sequential()\n","model2.add(resnet)\n","model2.add(layers.Flatten())\n","model2.add(Dense(512,activation='relu'))\n","model2.add(layers.Dropout(0.5))\n","model2.add(Dense(256,activation='relu'))\n","model2.add(layers.Dropout(0.5))\n","model2.add(Dense(104,activation='softmax'))\n","model2.compile(optimizer=optimizers.Adam(learning_rate=0.001),\n","    loss = 'sparse_categorical_crossentropy',\n","    metrics=['sparse_categorical_accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TI3BDOS_-Au0"},"outputs":[],"source":["model2_history = model2.fit(training_dataset, \n","          steps_per_epoch=STEPS_PER_EPOCH, \n","          epochs=EPOCHS,callbacks=callbacks_list, \n","          validation_data=validation_dataset)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","provenance":[],"mount_file_id":"1RUqftcYvPZG8ryua9RYTPRSgGF8inv05","authorship_tag":"ABX9TyO0lru+4GHeME7fLoHCbGGl"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}